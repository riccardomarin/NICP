hydra:
  run:
    dir: .
  sweep:
    dir: .
    subdir: .
  launcher:
    _target_: hydra._internal.core_plugins.basic_launcher.BasicLauncher
  sweeper:
    _target_: hydra._internal.core_plugins.basic_sweeper.BasicSweeper
    max_batch_size: null
    params: null
  help:
    app_name: ${hydra.job.name}
    header: '${hydra.help.app_name} is powered by Hydra.

      '
    footer: 'Powered by Hydra (https://hydra.cc)

      Use --hydra-help to view Hydra specific help

      '
    template: '${hydra.help.header}

      == Configuration groups ==

      Compose your configuration from those groups (group=option)


      $APP_CONFIG_GROUPS


      == Config ==

      Override anything in the config (foo.bar=value)


      $CONFIG


      ${hydra.help.footer}

      '
  hydra_help:
    template: 'Hydra (${hydra.runtime.version})

      See https://hydra.cc for more info.


      == Flags ==

      $FLAGS_HELP


      == Configuration groups ==

      Compose your configuration from those groups (For example, append hydra/job_logging=disabled
      to command line)


      $HYDRA_CONFIG_GROUPS


      Use ''--cfg hydra'' to Show the Hydra config.

      '
    hydra_help: ???
  hydra_logging:
    version: 1
    root: null
    disable_existing_loggers: false
  job_logging:
    version: 1
    root: null
    disable_existing_loggers: false
  env: {}
  mode: MULTIRUN
  searchpath: []
  callbacks: {}
  output_subdir: null
  overrides:
    hydra:
    - hydra.mode=MULTIRUN
    task:
    - core.tags=[SCAPE]
    - train.trainer.max_epochs=50
    - nn.data.batch_size.train=2,4,8
  job:
    name: run
    chdir: null
    override_dirname: core.tags=[SCAPE],nn.data.batch_size.train=2,4,8,train.trainer.max_epochs=50
    id: ???
    num: ???
    config_name: default
    env_set:
      WANDB_START_METHOD: thread
      WANDB_DIR: ${oc.env:PROJECT_ROOT}
    env_copy: []
    config:
      override_dirname:
        kv_sep: '='
        item_sep: ','
        exclude_keys: []
  runtime:
    version: 1.3.2
    version_base: '1.1'
    cwd: /mnt/sdb/backup/System Volume Information/LVD_CLEAN_2/lvd_templ
    config_sources:
    - path: hydra.conf
      schema: pkg
      provider: hydra
    - path: /mnt/sdb/backup/System Volume Information/LVD_CLEAN_2/lvd_templ/conf_ifnet
      schema: file
      provider: main
    - path: ''
      schema: structured
      provider: schema
    output_dir: ???
    choices:
      train: default
      nn: default
      hydra: default
      hydra/env: default
      hydra/callbacks: null
      hydra/job_logging: none
      hydra/hydra_logging: none
      hydra/hydra_help: default
      hydra/help: default
      hydra/sweeper: basic
      hydra/launcher: basic
      hydra/output: default
  verbose: false
nn:
  data:
    _target_: lvd_templ.data.datamodule_AMASS.MyDataModule
    overfit: false
    gpus: ${train.trainer.gpus}
    datasets:
      version: V1_SV1_T9
      n_data: 0
      n_points: 0
      template: false
      red_factor: 10
      type: occ_dist
      res: 64
      train:
        _target_: lvd_templ.data.dataset_AMASS_occ.AMASSDataset
        version: ${nn.data.datasets.version}
        n_data: ${nn.data.datasets.n_data}
        n_points: ${nn.data.datasets.n_points}
        template: ${nn.data.datasets.template}
        seed_idxs: 0
        fine_std: 0.05
        n_uniform: 400
        n_fine_sampled: 1800
        red_factor: ${nn.data.datasets.red_factor}
        type: ${nn.data.datasets.type}
        res: ${nn.data.datasets.res}
        locality: ${nn.module.locality}
        v_locality: 0
        segm: ${nn.module.segm}
        coeffs: false
        unsupervised: false
    num_workers:
      train: 24
      val: 16
      test: 16
    batch_size:
      train: 8
      val: 4
      test: 4
  module:
    _target_: lvd_templ.pl_modules.pl_module_ifnet.LightUniversal
    paradigm: LVD
    n_basis: 20
    n_desc: 40
    n_layers: 10
    size_layers: 256
    n_points: ${nn.data.datasets.train.n_points}
    template: ${nn.data.datasets.template}
    locality: 0
    fine_std: ${nn.data.datasets.train.fine_std}
    n_uniform: ${nn.data.datasets.train.n_uniform}
    n_fine_sampled: ${nn.data.datasets.train.n_fine_sampled}
    selfsup: false
    grad: 100
    clamp: 0.05
    clamp_style: 1
    gpus: ${train.trainer.gpus}
    powerup: 1
    power_factor: 2
    segm: 16
    positional: 0
    unsupervised: ${nn.data.datasets.train.unsupervised}
    onlydir: false
    optimizer:
      _target_: torch.optim.Adam
      lr: 0.0001
      betas:
      - 0.9
      - 0.999
      eps: 1.0e-08
      weight_decay: 0
train:
  seed_index: 3407
  deterministic: false
  trainer:
    fast_dev_run: false
    gpus: 1
    precision: 32
    max_epochs: 50
    max_steps: 1000000000
    accumulate_grad_batches: 1
    num_sanity_val_steps: 2
    gradient_clip_val: 10.0
    val_check_interval: 1.0
    deterministic: ${train.deterministic}
  restore:
    ckpt_or_run_path: null
    mode: continue
  monitor:
    metric: loss/val
    mode: min
  callbacks:
  - _target_: pytorch_lightning.callbacks.EarlyStopping
    patience: 100000
    verbose: false
    monitor: ${train.monitor.metric}
    mode: ${train.monitor.mode}
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    save_top_k: 1
    verbose: false
    monitor: ${train.monitor.metric}
    mode: ${train.monitor.mode}
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
    log_momentum: false
  - _target_: pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar
    refresh_rate: 20
  logging:
    upload:
      run_files: true
      source: true
    logger:
      _target_: pytorch_lightning.loggers.WandbLogger
      project: ${core.project_name}
      entity: null
      log_model: ${..upload.run_files}
      mode: online
      tags: ${core.tags}
    wandb_watch:
      log: all
      log_freq: 100
core:
  project_name: matchAMASS
  storage_dir: ${oc.env:PROJECT_ROOT}/storage
  version: 0.0.1
  tags:
  - SCAPE
